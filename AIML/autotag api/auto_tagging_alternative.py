# -*- coding: utf-8 -*-
"""Copy of Extended_Auto_tagging_Alternative.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X_xpJ8h42Iec9HyEw9QZtGtgyLBqj4hh
"""

import random
import pandas as pd

domains = {
    # 1â€“10 : Tech + AI + Science
    "technology": ["AI", "machine learning", "data science", "robotics", "cloud computing", "blockchain", "IoT", "quantum computing"],
    "software_dev": ["python", "java", "web development", "API design", "debugging", "frontend", "backend"],
    "cybersecurity": ["hacking", "malware", "network security", "phishing", "encryption"],
    "data_engineering": ["ETL pipelines", "big data", "Spark", "Kafka", "data lakes"],
    "mobile_dev": ["android", "ios", "flutter", "react native", "mobile apps"],
    "gaming": ["PC games", "console", "eSports", "RPG", "strategy games"],
    "ai_ethics": ["AI fairness", "bias", "privacy", "responsible AI"],
    "hardware": ["CPUs", "GPUs", "Raspberry Pi", "microcontrollers"],
    "quantum": ["quantum mechanics", "quantum computing", "Qiskit"],
    "space_tech": ["rockets", "satellites", "SpaceX", "NASA", "Mars mission"],

    # 11â€“20 : Business + Finance
    "finance": ["stock market", "investment", "crypto", "mutual funds", "banking"],
    "economy": ["inflation", "GDP", "recession", "interest rates"],
    "startups": ["funding", "pitch decks", "growth hacking", "founder stories"],
    "marketing": ["SEO", "digital marketing", "branding", "ads"],
    "productivity": ["time management", "routines", "work-life balance"],
    "leadership": ["team building", "management", "decision making"],
    "sales": ["CRM", "sales funnels", "lead generation"],
    "ecommerce": ["Shopify", "dropshipping", "product listings"],
    "real_estate": ["property investment", "housing market", "mortgage"],
    "business_analytics": ["KPIs", "dashboard", "data visualization"],

    # 21â€“30 : Health + Lifestyle
    "health": ["mental health", "fitness", "diet", "stress", "yoga"],
    "nutrition": ["healthy food", "vitamins", "smoothies", "meal plans"],
    "medicine": ["diseases", "treatments", "surgeries", "pharma"],
    "covid": ["pandemic", "vaccine", "immunity"],
    "lifestyle": ["travel", "home decor", "minimalism", "self care"],
    "fashion": ["style", "outfits", "accessories", "trends"],
    "beauty": ["skincare", "makeup", "hair care"],
    "parenting": ["kids", "education", "tips"],
    "pets": ["dogs", "cats", "animal health"],
    "food": ["recipes", "restaurants", "cuisine"],

    # 31â€“40 : Entertainment + Arts
    "movies": ["Hollywood", "Bollywood", "reviews", "trailers"],
    "music": ["pop", "rock", "hip hop", "concerts"],
    "tv_shows": ["Netflix", "Prime", "series", "drama"],
    "books": ["novels", "authors", "fiction", "reviews"],
    "education": ["learning", "exams", "online courses", "MOOCs"],
    "science": ["physics", "chemistry", "biology", "research"],
    "history": ["ancient", "wars", "civilizations"],
    "philosophy": ["ethics", "logic", "existentialism"],
    "art": ["painting", "sculpture", "digital art"],
    "photography": ["DSLR", "composition", "editing"],

    # 41â€“50 : Society + Current Affairs
    "politics": ["elections", "policy", "governance"],
    "environment": ["climate change", "sustainability", "pollution"],
    "law": ["human rights", "legal advice", "justice"],
    "religion": ["faith", "beliefs", "spirituality"],
    "psychology": ["motivation", "behavior", "cognition"],
    "sports": ["football", "cricket", "basketball", "tennis"],
    "news": ["breaking news", "global events"],
    "travel": ["destinations", "guides", "solo travel"],
    "automobile": ["cars", "EVs", "racing", "maintenance"],
    "career": ["interview tips", "resume writing", "job search"],

    # 51â€“60 : Food + Cooking + Drinks
    "cooking": ["baking", "grilling", "soups", "desserts", "meal prep"],
    "world_cuisine": ["Italian", "Mexican", "Chinese", "Indian"],
    "beverages": ["coffee", "tea", "mocktails", "cocktails"],
    "street_food": ["burgers", "tacos", "chaat", "shawarma"],
    "home_cooking": ["easy meals", "family dinner", "vegan"],
    "healthy_cooking": ["low carb", "salads", "protein meals"],
    "kitchen_tips": ["hacks", "tools", "storage"],
    "baking": ["cakes", "cookies", "bread", "frosting"],
    "desserts": ["ice cream", "brownies", "pastries"],
    "snacks": ["chips", "popcorn", "party snacks"],

    # 61â€“70 : Travel + Culture
    "adventure": ["hiking", "camping", "skydiving", "trekking"],
    "world_travel": ["Europe", "Asia", "Africa", "America tours"],
    "budget_travel": ["cheap flights", "hostels", "hacks"],
    "luxury_travel": ["resorts", "cruise", "villas"],
    "backpacking": ["solo trips", "beaches", "mountains"],
    "culture": ["festivals", "heritage", "traditions"],
    "phototravel": ["travel photography", "nature shots"],
    "city_life": ["transport", "nightlife", "events"],
    "travel_safety": ["insurance", "documents", "tips"],
    "ecotourism": ["wildlife", "green travel"],

    # 71â€“80 : Hobbies + DIY
    "gardening": ["plants", "flowers", "balcony garden"],
    "crafts": ["DIY", "origami", "handmade gifts"],
    "music_instruments": ["guitar", "piano", "violin"],
    "writing": ["poetry", "journaling", "story writing"],
    "language_learning": ["English", "Spanish", "Japanese"],
    "board_games": ["chess", "ludo", "scrabble"],
    "fitness_hobby": ["yoga", "cycling", "gym"],
    "collectibles": ["stamps", "coins", "cards"],
    "volunteering": ["charity", "fundraising"],
    "home_projects": ["repairs", "furniture", "painting"],

    # 81â€“90 : Tech Career + Emerging Trends
    "ai_research": ["transformers", "NLP", "computer vision"],
    "career_growth": ["upskilling", "courses", "mentorship"],
    "freelancing": ["clients", "contracts", "remote work"],
    "web3": ["NFTs", "smart contracts", "DAOs"],
    "metaverse": ["VR", "AR", "virtual world"],
    "robotics_future": ["automation", "robots"],
    "quant_finance": ["algorithmic trading", "hedge funds"],
    "green_energy": ["solar", "wind", "sustainability"],
    "space_exploration": ["Mars", "astronomy"],
    "tech_trends": ["AI tools", "startups", "innovation"],

    # 91â€“100 : Moods + Emotions + Casual Texts
    "moods": ["happy", "sad", "excited", "bored", "motivated"],
    "emotions": ["love", "anger", "fear", "joy", "surprise"],
    "daily_life": ["morning vibes", "office day", "lazy weekend"],
    "motivation": ["success", "positivity", "quotes"],
    "relationships": ["friendship", "breakup", "trust", "dating"],
    "self_improvement": ["habits", "goals", "confidence"],
    "humor": ["funny", "sarcasm", "puns"],
    "memes": ["relatable", "trend", "viral"],
    "casual_chat": ["random thoughts", "life updates", "rants"],
    "feel_good": ["grateful", "peaceful", "calm"],

    # 101â€“120 : Random & Cultural Domains
    "festivals": [
    "Diwali", "Holi", "Eid", "Christmas", "Navratri", "Durga Puja",
    "Ganesh Chaturthi", "Pongal", "Onam", "Baisakhi", "Makar Sankranti",
    "Janmashtami", "Raksha Bandhan", "Karva Chauth", "Lohri", "Gudi Padwa",
    "Bihu", "Mahashivratri", "Ram Navami", "Dussehra", "Ugadi",
    "Vishu", "Chhath Puja", "Eid al-Adha", "Eid ul-Fitr", "Buddha Purnima",
    "Guru Nanak Jayanti", "Parsi New Year", "Christmas", "New Year", "Independence Day",
    "Republic Day", "Gandhi Jayanti", "Childrenâ€™s Day", "Teachersâ€™ Day", "Friendship Day"],
    "pets_care": ["training", "grooming", "feeding"],
    "nature": ["rain", "forest", "mountains", "beach"],
    "weather": ["sunny", "rainy", "winter", "summer"],
    "college_life": ["assignments", "friends", "parties"],
    "office_life": ["deadlines", "team meetings"],
    "quotes": ["inspirational", "life lessons"],
    "digital_creators": ["YouTubers", "influencers"],
    "anime": ["Naruto", "One Piece", "Attack on Titan"],
    "kpop": ["BTS", "BLACKPINK", "K-drama"],
    "casual_vibes": ["just chilling", "coffee mood", "no filter"],
    "weekend_feels": ["Friday night", "Sunday brunch"],
    "music_vibes": ["playlist", "chill beats", "dance mode"],
    "personal_growth": ["mindfulness", "discipline", "journaling"],
    "college_memes": ["assignments", "exams", "hostel life"],
    "adulting": ["bills", "laundry", "budgeting"],
    "nostalgia": ["childhood", "memories", "old days"],
    "random_thoughts": ["shower thoughts", "deep talks"],
    "meme_culture": ["funny posts", "Twitter trends"],
    "emotional_quotes": ["heartbreak", "self love", "healing"],

    # 61â€“70 : Food Varieties + Dishes + Ingredients
    "indian_food": [
        "biryani", "butter chicken", "paneer tikka", "masala dosa", "idli", "samosa",
        "chole bhature", "rajma chawal", "tandoori roti", "dal makhani", "rogan josh"
    ],
    "continental_food": [
        "pasta", "lasagna", "risotto", "pizza", "steak", "grilled chicken", "mashed potatoes"
    ],
    "asian_food": [
        "sushi", "ramen", "dumplings", "noodles", "tempura", "teriyaki", "kimchi", "bibimbap"
    ],
    "fast_food": [
        "burgers", "fries", "hot dogs", "sandwiches", "fried chicken", "nuggets"
    ],
    "dessert_specials": [
        "chocolate cake", "cheesecake", "gulab jamun", "rasgulla", "brownies", "donuts",
        "tiramisu", "pudding", "cupcakes"
    ],
    "beverages_special": [
        "cold coffee", "espresso", "milkshake", "smoothie", "green tea", "lemonade",
        "iced tea", "mojito", "latte", "frappuccino"
    ],
    "spices_and_ingredients": [
        "turmeric", "cumin", "cardamom", "chili powder", "ginger", "garlic", "coriander",
        "saffron", "mustard seeds", "cloves"
    ],
    "breakfast_items": [
        "pancakes", "omelette", "toast", "cereal", "paratha", "upma", "poha", "bagel", "waffles"
    ],
    "street_food_india": [
        "pani puri", "vada pav", "pav bhaji", "kathi roll", "momos", "pakora", "bhel puri"
    ],
    "regional_food": [
        "Hyderabadi biryani", "Bengali fish curry", "Punjabi lassi", "Goan prawn curry",
        "Kerala appam", "Gujarati dhokla", "Rajasthani dal bati churma"
    ],

    "mediterranean_food": ["hummus", "falafel", "shawarma", "tabbouleh", "pita bread"],
    "american_food": ["mac and cheese", "buffalo wings", "barbecue ribs", "apple pie"],
    "french_cuisine": ["croissant", "baguette", "soufflÃ©", "ratatouille", "crepes"],
    "dessert_trends": ["vegan desserts", "sugar-free sweets", "ice cream rolls", "frozen yogurt"]

}

templates = [
    # Informative / Educational
    "Letâ€™s break down the basics of {topic}.",
    "Hereâ€™s a quick overview of {topic}.",
    "Simplifying {topic} for everyone.",
    "A step-by-step guide to understanding {topic}.",
    "What makes {topic} so interesting?",
    "The hidden side of {topic} nobody talks about.",
    "Letâ€™s decode {topic} in the easiest way possible.",
    "Ever wondered how {topic} actually works?",
    "All you need to know about {topic} in one post.",
    "Understanding {topic} â€” made simple.",

    # Conversational / Relatable
    "Soâ€¦ letâ€™s talk about {topic}.",
    "Anyone else canâ€™t stop thinking about {topic}? ðŸ˜…",
    "This oneâ€™s for everyone who loves {topic}.",
    "Can we take a moment to appreciate {topic}?",
    "You ever just randomly think about {topic}?",
    "Raise your hand if {topic} is part of your daily life ðŸ™Œ",
    "Someone had to say it â€” {topic} is underrated!",
    "Tell me your thoughts on {topic} ðŸ‘‡",
    "Who else agrees {topic} makes life better?",
    "No one: absolutely no one: me talking about {topic} ðŸ˜‚",

    # Motivational / Inspirational
    "Keep growing â€” one {topic} at a time ðŸŒ±",
    "Dream big. Start small. Do {topic}.",
    "{topic} taught me patience and persistence ðŸ’ª",
    "The journey through {topic} is worth every step.",
    "Believe in yourself â€” {topic} is just the start.",
    "Sometimes progress looks like small wins in {topic}.",
    "Hard work + consistency = {topic} success ðŸ”¥",
    "You donâ€™t have to be perfect, just passionate about {topic}.",
    "Stay curious, stay creative, stay {topic}.",
    "You become what you practice â€” like {topic}.",

    # Emotional / Mood-based
    "Feeling all the feels with {topic} today ðŸ˜Œ",
    "Some days are just made for {topic}.",
    "Canâ€™t explain it, but {topic} just hits different today.",
    "Finding calm in {topic} ðŸŒ¸",
    "{topic} makes me feel alive.",
    "Peaceful moments surrounded by {topic}.",
    "Nothing but good vibes and {topic}.",
    "My comfort zone? Definitely {topic}.",
    "Happiness = coffee + {topic} â˜•ðŸ’«",
    "A little {topic} therapy for the soul.",

    # Travel / Adventure
    "Wanderlust powered by {topic} âœˆï¸",
    "Exploring new places and discovering {topic}.",
    "Chasing sunsets and {topic} vibes ðŸŒ…",
    "Adventure starts with {topic}.",
    "Take me where {topic} feels endless ðŸŒ",
    "Lost in {topic} but loving every moment.",
    "New city, same love for {topic}.",
    "Journey > destination, always â€” especially with {topic}.",
    "Road trips and {topic} playlists ðŸ’¿",
    "Because lifeâ€™s too short to skip {topic}.",

    # Food / Cooking / Lifestyle
    "Cooked something special â€” all about {topic} ðŸ½ï¸",
    "Weekend cravings: {topic} edition ðŸ¤¤",
    "When food meets art â€” thatâ€™s {topic}.",
    "Comfort food = {topic} for me ðŸ²",
    "Kitchen experiments gone right â€” thanks to {topic}.",
    "Canâ€™t stop drooling over {topic} ðŸ˜‹",
    "Sunday vibes = {topic} and chill.",
    "Adding a little spice of {topic} to my day ðŸŒ¶ï¸",
    "Healthy, tasty, and all about {topic} ðŸ¥—",
    "Homemade happiness with {topic} ðŸ¡",

    # Aesthetic / Creative
    "Soft tones and {topic} aesthetics ðŸŽ¨",
    "Let your creativity flow with {topic}.",
    "The beauty of {topic} lies in the details.",
    "Chasing light and {topic} vibes âœ¨",
    "Made something beautiful out of {topic}.",
    "Art meets heart through {topic}.",
    "Colors inspired by {topic}.",
    "Minimalism + {topic} = perfection.",
    "Capturing moments that scream {topic}.",
    "Creating magic through {topic} âœ¨",

    # Social Media / Trendy
    "POV: Youâ€™re scrolling and find this {topic} post ðŸ‘€",
    "Random post about {topic} because why not ðŸ˜Ž",
    "This oneâ€™s trending: {topic} ðŸ”¥",
    "That â€˜just becauseâ€™ {topic} moment.",
    "New post, same obsession â€” {topic}.",
    "Caught myself talking about {topic} again ðŸ˜…",
    "Your daily dose of {topic}.",
    "Canâ€™t stop scrolling through {topic} posts.",
    "Lowkey obsessed with {topic} rn ðŸ’«",
    "#Aesthetic #Mood #Just{topic}Things",

    # Events / Festivals / Celebrations
    "Wishing everyone a joyful {topic} âœ¨",
    "Celebrating the spirit of {topic} with love and light ðŸª”",
    "Festive vibes only â€” itâ€™s {topic} season! ðŸŽ‰",
    "Let the colors of {topic} brighten your day ðŸŒˆ",
    "Happy {topic}! Let the celebrations begin ðŸ¥³",
    "Filled with gratitude and {topic} spirit ðŸ™",
    "May your {topic} be as bright as your smile ðŸŒŸ",
    "Enjoying every bit of {topic} with family and friends â¤ï¸",
    "Spreading positivity and {topic} joy all around ðŸŽŠ",
    "Traditions + togetherness = {topic} magic ðŸ’«",

    # Moods / Personality
    "Current mood: {topic}.",
    "Just vibing with {topic} energy âœ¨",
    "Unbothered. Hydrated. Focused on {topic}.",
    "Main character energy featuring {topic}.",
    "Too glam to give a damn â€” just {topic}.",
    "Peace, coffee, and {topic} kind of morning â˜•",
    "Sundays are for {topic} and self-care ðŸ§˜",
    "Sometimes silence speaks {topic}.",
    "Feeling extra today â€” must be the {topic} vibe ðŸ’…",
    "Manifesting good energy and {topic}.",

    # Promotional / Announcement
    "Launching something exciting around {topic}! ðŸš€",
    "New blog out now â€” all about {topic}.",
    "Just dropped a new video on {topic}! ðŸŽ¥",
    "Super thrilled to share my work on {topic}.",
    "Weâ€™re hiring! Join our {topic}-driven team ðŸ’¼",
    "Big reveal: our next project involves {topic}.",
    "Just went live talking about {topic}.",
    "You donâ€™t want to miss this {topic} update ðŸ””",
    "Officially partnering for {topic} collaborations ðŸ¤",
    "Proud moment â€” our {topic} journey continues ðŸ’«",

    # Quotes / Deep Thoughts
    "â€˜In the end, everything connects through {topic}.â€™",
    "â€˜You find yourself in the silence of {topic}.â€™",
    "â€˜Happiness is not a destination, itâ€™s {topic}.â€™",
    "â€˜What you seek is already within â€” just like {topic}.â€™",
    "â€˜{topic} teaches us patience and balance.â€™",
    "â€˜Life feels complete with a little {topic}.â€™",
    "â€˜Be still. Feel. Breathe. {topic}.â€™",
    "â€˜Where words fail, {topic} speaks.â€™",
    "â€˜Every story has a {topic} hidden in it.â€™",
    "â€˜The art of living is learning from {topic}.â€™"
]

!pip install -q sentence-transformers git+https://github.com/trent-b/iterative-stratification xgboost joblib


import random, json, os, ast
import numpy as np, pandas as pd
from collections import Counter
from tqdm.auto import tqdm

from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.model_selection import train_test_split
from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit

from sentence_transformers import SentenceTransformer

from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb

from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, average_precision_score
import joblib

RND = 42
random.seed(RND)
np.random.seed(RND)

def generate_caption():
    domain = random.choice(list(domains.keys()))
    topic = random.choice(domains[domain])
    template = random.choice(templates)
    caption = template.format(topic=topic)
    return f"Domain: {domain.capitalize()}\n Topic: {topic}\n Caption: {caption}"

for i in range(10):
    print(f"\n Caption {i+1}")
    print(generate_caption())

SAMPLES_PER_DOMAIN = 500
OUTPUT_CSV = "synthetic_autotag_mega_dataset.csv"

posts, tags = [], []
bad_templates = []

for domain, topics in domains.items():
    for _ in range(SAMPLES_PER_DOMAIN):
        topic = random.choice(topics)
        template = random.choice(templates)
        try:
            if "{topic}" in template:
                caption = template.format(topic=topic)
            elif "{}" in template:
                caption = template.format(topic)
            else:
                caption = f"{template} {topic}"
        except Exception as e:
            bad_templates.append((template, str(e)))
            caption = f"{template} {topic}"

        posts.append(caption.strip())
        tags.append([domain.lower(), str(topic).lower().replace(" ", "_")])

df = pd.DataFrame({"post_caption": posts, "tags": tags})
df["post_caption"] = df["post_caption"].astype(str).str.strip()
df = df[df["post_caption"].str.len() > 0].drop_duplicates(subset="post_caption").sample(frac=1, random_state=RND).reset_index(drop=True)

print("Generated rows (after dedup):", len(df))
if bad_templates:
    print("Some templates caused formatting errors. Example:", bad_templates[:5])

df.to_csv(OUTPUT_CSV, index=False)
print("Saved synthetic dataset to:", OUTPUT_CSV)

# Load generated dataset
df = pd.read_csv(OUTPUT_CSV)


def safe_eval_tags(x):
    if isinstance(x, str):
        try:
            v = ast.literal_eval(x)
            if isinstance(v, (list,tuple)):
                return [str(t).strip() for t in v]
            return [str(v).strip()]
        except Exception:
            # if stored as "a,b" or similar
            if "," in x:
                return [s.strip() for s in x.split(",") if s.strip()]
            return [x.strip()]
    elif isinstance(x, (list,tuple)):
        return [str(t).strip() for t in x]
    else:
        return []

df['tags'] = df['tags'].apply(safe_eval_tags)

# Normalize text
df['post_caption'] = df['post_caption'].astype(str).str.strip()
df = df[df['post_caption'].str.len() > 0].reset_index(drop=True)

# Inspect tag distribution
all_tags = [t.lower().replace(" ", "_") for tags in df['tags'] for t in tags]
tag_counts = Counter(all_tags)
print("Unique raw tags:", len(tag_counts))
print("Top 20 tags:", tag_counts.most_common(20))


MIN_FREQ = 5
valid_tags = {t for t,c in tag_counts.items() if c >= MIN_FREQ}

def prune_tags(tag_list):
    tnorm = [t.lower().replace(" ", "_") for t in tag_list]
    return [t for t in tnorm if t in valid_tags]

df['tags_pruned'] = df['tags'].apply(prune_tags)
df = df[df['tags_pruned'].map(len) > 0].reset_index(drop=True)
print("Rows after pruning rare tags:", len(df))

mlb = MultiLabelBinarizer(sparse_output=False)
Y = mlb.fit_transform(df['tags_pruned'])
print("Number of labels after pruning:", len(mlb.classes_))
print("Sample labels:", mlb.classes_[:30])

X_texts = df['post_caption'].tolist()
msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.20, random_state=RND)
train_idx, test_idx = next(msss.split(X_texts, Y))

# split test -> val + test
test_idx, val_idx = train_test_split(test_idx, test_size=0.5, random_state=RND)

X_train_texts = [X_texts[i] for i in train_idx]; Y_train = Y[train_idx]
X_val_texts   = [X_texts[i] for i in val_idx];   Y_val   = Y[val_idx]
X_test_texts  = [X_texts[i] for i in test_idx];  Y_test  = Y[test_idx]

print("Sizes -> train, val, test:", len(X_train_texts), len(X_val_texts), len(X_test_texts))

SBERT_MODEL = "all-MiniLM-L6-v2"   # change to "all-mpnet-base-v2" for higher quality
device = "cuda" if __import__('torch').cuda.is_available() else "cpu"
print("Using device:", device, "SBERT model:", SBERT_MODEL)

sbert = SentenceTransformer(SBERT_MODEL, device=device)

# encode (batches)
X_train_emb = sbert.encode(X_train_texts, batch_size=128, show_progress_bar=True, convert_to_numpy=True)
X_val_emb   = sbert.encode(X_val_texts,   batch_size=128, show_progress_bar=True, convert_to_numpy=True)
X_test_emb  = sbert.encode(X_test_texts,  batch_size=128, show_progress_bar=True, convert_to_numpy=True)

print("Embeddings shapes:", X_train_emb.shape, X_val_emb.shape, X_test_emb.shape)

def get_proba_matrix(model, X):
    """
    Return probabilities shape (n_samples, n_labels)
    Works with OneVsRestClassifier and fallback to decision_function.
    """
    try:
        proba = model.predict_proba(X)
        if isinstance(proba, list):
            arr = np.vstack([p[:,1] if p.ndim==2 and p.shape[1]>1 else p.ravel() for p in proba]).T
            return arr
        else:
            return proba
    except Exception:
        try:
            scores = model.decision_function(X)
            from scipy.special import expit
            return expit(scores)
        except Exception as e:
            raise RuntimeError("Model does not support predict_proba/decision_function: " + str(e))

models = {}

print("Training Logistic Regression (One-vs-Rest)...")
lr = OneVsRestClassifier(LogisticRegression(max_iter=2000, solver='saga', n_jobs=-1, C=1.0))
lr.fit(X_train_emb, Y_train)
models['logreg'] = lr

print("Training complete.")

from sklearn.metrics import f1_score

def tune_thresholds(probs_val, Y_val, n_steps=30):
    n_labels = Y_val.shape[1]
    thresholds = np.ones(n_labels) * 0.5
    for i in range(n_labels):
        best_f1 = -1.0
        best_t = 0.5
        scores = probs_val[:, i]
        for t in np.linspace(0.01, 0.99, n_steps):
            preds = (scores >= t).astype(int)
            f1 = f1_score(Y_val[:, i], preds, zero_division=0)
            if f1 > best_f1:
                best_f1 = f1
                best_t = t
        thresholds[i] = best_t
    return thresholds

thresholds = {}
proba_val = {}
for name, model in models.items():
    print("Computing val probs for", name)
    pv = get_proba_matrix(model, X_val_emb)
    proba_val[name] = pv
    print("Tuning thresholds for", name)
    thresholds[name] = tune_thresholds(pv, Y_val, n_steps=30)
    print(name, " mean threshold:", thresholds[name].mean())

from sklearn.metrics import precision_score, recall_score

def evaluate_with_thresholds(model, X_emb, Y_true, thresholds, label_names, top_k=10):
    probs = get_proba_matrix(model, X_emb)
    preds = (probs >= thresholds.reshape(1,-1)).astype(int)
    micro_f1 = f1_score(Y_true, preds, average='micro', zero_division=0)
    macro_f1 = f1_score(Y_true, preds, average='macro', zero_division=0)
    micro_p = precision_score(Y_true, preds, average='micro', zero_division=0)
    micro_r = recall_score(Y_true, preds, average='micro', zero_division=0)
    print(f"micro_f1:{micro_f1:.4f} macro_f1:{macro_f1:.4f} precision:{micro_p:.4f} recall:{micro_r:.4f}")
    print("\nPer-label small report (first {} labels):".format(top_k))
    print(classification_report(Y_true[:, :top_k], preds[:, :top_k], target_names=label_names[:top_k], zero_division=0))
    # mAP
    ap_list = []
    for i in range(Y_true.shape[1]):
        try:
            ap_list.append(average_precision_score(Y_true[:, i], probs[:, i]))
        except Exception:
            ap_list.append(0.0)
    print("Mean Average Precision (mAP):", np.mean(ap_list))
    return preds, probs, ap_list

results = {}
for name, model in models.items():
    print("\n=== Evaluating", name, "===")
    preds, probs, ap_list = evaluate_with_thresholds(model, X_test_emb, Y_test, thresholds[name], mlb.classes_, top_k=10)
    results[name] = {"preds": preds, "probs": probs, "ap": ap_list}

w = {'logreg':1.0}
proba_val_ens = w['logreg']*proba_val['logreg']
thresholds['ensemble'] = tune_thresholds(proba_val_ens, Y_val, n_steps=30)
print("Ensemble mean threshold:", thresholds['ensemble'].mean())

# ensemble on test
proba_test_ens = w['logreg']*results['logreg']['probs']
preds_ens = (proba_test_ens >= thresholds['ensemble'].reshape(1,-1)).astype(int)
print("\nEnsemble micro_f1:", f1_score(Y_test, preds_ens, average='micro', zero_division=0))
print("Ensemble micro_precision:", precision_score(Y_test, preds_ens, average='micro', zero_division=0))
print("Ensemble micro_recall:", recall_score(Y_test, preds_ens, average='micro', zero_division=0))

OUT_DIR = "autotag_artifacts"
os.makedirs(OUT_DIR, exist_ok=True)

joblib.dump(mlb, os.path.join(OUT_DIR, "mlb.joblib"))
joblib.dump(models['logreg'], os.path.join(OUT_DIR, "logreg.joblib"))

with open(os.path.join(OUT_DIR, "sbert_model.json"), "w") as f:
    json.dump({"name": SBERT_MODEL, "device": device}, f)
joblib.dump(thresholds, os.path.join(OUT_DIR, "thresholds.joblib"))
print("Saved artifacts to:", OUT_DIR)

def load_artifacts(base=OUT_DIR):
    mlb = joblib.load(os.path.join(base, "mlb.joblib"))
    logreg = joblib.load(os.path.join(base, "logreg.joblib"))

    with open(os.path.join(base, "sbert_model.json"), "r") as f:
        s = json.load(f)
    thresholds = joblib.load(os.path.join(base, "thresholds.joblib"))
    sbert_local = SentenceTransformer(s["name"], device=device)
    return mlb, logreg,sbert_local, thresholds

mlb_l, logreg_l,  sbert_l, thresholds_all = load_artifacts()

def predict_tags_texts(texts, model_name='ensemble', top_k=5):
    cleaned = [str(t).strip() for t in texts]
    emb = sbert_l.encode(cleaned, convert_to_numpy=True)
    p_lr = get_proba_matrix(logreg_l, emb)

    proba_mix = w['logreg']*p_lr
    if model_name=='ensemble':
        probs = proba_mix; th = thresholds_all['ensemble']
    else:
        probs = {'logreg':p_lr}[model_name]
        th = thresholds_all[model_name]
    preds_bin = (probs >= th.reshape(1,-1)).astype(int)
    outputs = []
    for i in range(len(texts)):
        label_idxs = np.where(preds_bin[i]==1)[0]
        if len(label_idxs)==0:
            label_idxs = np.argsort(-probs[i])[:top_k]
        labels = [mlb_l.classes_[j] for j in label_idxs]
        scores = [float(probs[i,j]) for j in label_idxs]
        outputs.append(list(zip(labels, scores)))
    return outputs

print(predict_tags_texts([ "Having kids is an extreme sports too hahaha !"]))